# -*- coding: utf-8 -*-
"""
═══════════════════════════════════════════════════════════════════════════
AGENTE BIBLIOTECÁRIO - ARCHIVUS
Ávila Inc / Ávila Ops
═══════════════════════════════════════════════════════════════════════════
Função:
  - Curadoria e governança de documentação corporativa
  - Backup automático e rotação de logs
- Verificação de integridade (SHA256)
  - Validação de estrutura de pastas
  - Remoção automática de arquivos não-oficiais
═══════════════════════════════════════════════════════════════════════════
"""

import os
import sys
import shutil
import hashlib
import json
import yaml
from pathlib import Path
from datetime import datetime, timedelta
import zipfile

# ═══════════════════════════════════════════════════════════════════════════
# CONFIGURAÇÕES
# ═══════════════════════════════════════════════════════════════════════════

SCRIPT_DIR = Path(__file__).resolve().parent
CONFIG_FILE = SCRIPT_DIR / "config.yaml"
WORKSPACE_ROOT = Path(r"C:\Users\nicol\OneDrive\Avila")

# Carregar configuração
with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
  CONFIG = yaml.safe_load(f)

# Paths
DOCS_PATH = WORKSPACE_ROOT / "Docs"
LOGS_PATH = WORKSPACE_ROOT / "Logs"
SCRIPTS_PATH = WORKSPACE_ROOT / "Scripts"
BACKUP_PATH = WORKSPACE_ROOT / "Shared" / "backups" / "archivus"
MANIFEST_FILE = SCRIPT_DIR / CONFIG['integrity']['manifest_file']

# Criar estrutura se não existir
BACKUP_PATH.mkdir(parents=True, exist_ok=True)
(LOGS_PATH / "Daily").mkdir(parents=True, exist_ok=True)
(LOGS_PATH / "Archive").mkdir(parents=True, exist_ok=True)

# ═══════════════════════════════════════════════════════════════════════════
# CLASSE PRINCIPAL
# ═══════════════════════════════════════════════════════════════════════════

class Archivus:
    """Agente Bibliotecário - Curadoria e Governança Documental"""
    
    def __init__(self):
        self.config = CONFIG
        self.log_file = LOGS_PATH / "Daily" / f"archivus_{datetime.now().strftime('%Y-%m-%d')}.log"
        self.report_data = {
      'timestamp': datetime.now().isoformat(),
            'actions': [],
    'metrics': {},
            'issues': []
 }
        
    def log(self, message: str, level: str = "INFO"):
        """Registra mensagem no log"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
 log_entry = f"[{timestamp}] [{level}] {message}"
      
    print(log_entry)
        
        with open(self.log_file, 'a', encoding='utf-8') as f:
          f.write(log_entry + "\n")
    
  # ═══════════════════════════════════════════════════════════════════════
    # VALIDAÇÃO DE ESTRUTURA
    # ═══════════════════════════════════════════════════════════════════════
    
    def validate_structure(self):
      """Valida estrutura de pastas e remove itens não-oficiais"""
        self.log("Iniciando validação de estrutura...", "INFO")
        
        removed_items = []
        
        # Validar Docs/Relatorios
        relatorios_path = DOCS_PATH / "Relatorios"
        if relatorios_path.exists():
        official_folders = CONFIG['official_structure']['Docs']['Relatorios']
          
            for item in relatorios_path.iterdir():
                if item.is_dir() and item.name not in official_folders:
        self.log(f"Removendo pasta não-oficial: {item.name}", "WARNING")
             shutil.rmtree(item)
        removed_items.append(str(item.relative_to(WORKSPACE_ROOT)))
    elif item.is_file():
         # Verificar se é arquivo oficial
               official_files = [
       "GUIA_RAPIDO.md",
    "IMPLEMENTACAO_COMPLETA.md",
"README.md",
              "Relatório Corporativo Executivo Versão Detalhada.md"
  ]
            if item.name not in official_files:
            self.log(f"Removendo arquivo não-oficial: {item.name}", "WARNING")
          item.unlink()
   removed_items.append(str(item.relative_to(WORKSPACE_ROOT)))
        
        self.report_data['actions'].append({
       'action': 'validate_structure',
            'removed_items': removed_items,
   'count': len(removed_items)
        })
        
        self.log(f"Validação concluída. {len(removed_items)} itens removidos.", "INFO")
        return removed_items
    
    # ═══════════════════════════════════════════════════════════════════════
    # ROTAÇÃO DE LOGS
    # ═══════════════════════════════════════════════════════════════════════
    
    def rotate_logs(self):
        """Move logs antigos para arquivo"""
        self.log("Iniciando rotação de logs...", "INFO")
        
        daily_logs = LOGS_PATH / "Daily"
      archive_logs = LOGS_PATH / "Archive"
  
        archive_after = self.config['monitored_paths']['logs']['archive_after_days']
        cutoff_date = datetime.now() - timedelta(days=archive_after)
        
        rotated_count = 0
        
     for log_file in daily_logs.glob("*.log"):
            file_mtime = datetime.fromtimestamp(log_file.stat().st_mtime)
     
    if file_mtime < cutoff_date:
 dest = archive_logs / log_file.name
            shutil.move(str(log_file), str(dest))
          rotated_count += 1
      self.log(f"Log arquivado: {log_file.name}", "INFO")
 
        self.report_data['actions'].append({
  'action': 'rotate_logs',
       'rotated_count': rotated_count
        })
        
        self.log(f"Rotação concluída. {rotated_count} logs arquivados.", "INFO")
    
    # ═══════════════════════════════════════════════════════════════════════
    # BACKUP DE SCRIPTS
    # ═══════════════════════════════════════════════════════════════════════
    
    def backup_scripts(self):
     """Cria backup compactado dos scripts"""
        self.log("Iniciando backup de scripts...", "INFO")
     
        timestamp = datetime.now().strftime("%Y-%m-%d_%H%M%S")
        backup_name = f"scripts_backup_{timestamp}.zip"
     backup_file = BACKUP_PATH / backup_name
        
        with zipfile.ZipFile(backup_file, 'w', zipfile.ZIP_DEFLATED) as zipf:
for root, dirs, files in os.walk(SCRIPTS_PATH):
            for file in files:
            file_path = Path(root) / file
    arcname = file_path.relative_to(WORKSPACE_ROOT)
            zipf.write(file_path, arcname)
        
        backup_size = backup_file.stat().st_size / 1024 / 1024  # MB
   
        self.report_data['actions'].append({
       'action': 'backup_scripts',
          'backup_file': backup_name,
    'size_mb': round(backup_size, 2)
        })
    
   self.log(f"Backup criado: {backup_name} ({backup_size:.2f} MB)", "INFO")
     
        # Limpar backups antigos
        self._cleanup_old_backups()
    
    def _cleanup_old_backups(self):
     """Remove backups antigos mantendo apenas os últimos N"""
        max_backups = self.config['backup']['max_backups_per_type']
        
        backups = sorted(BACKUP_PATH.glob("scripts_backup_*.zip"), key=lambda x: x.stat().st_mtime, reverse=True)
        
        for old_backup in backups[max_backups:]:
 old_backup.unlink()
       self.log(f"Backup antigo removido: {old_backup.name}", "INFO")
    
    # ═══════════════════════════════════════════════════════════════════════
    # VERIFICAÇÃO DE INTEGRIDADE
    # ═══════════════════════════════════════════════════════════════════════
    
    def calculate_hash(self, file_path: Path) -> str:
        """Calcula hash SHA256 de um arquivo"""
    sha256 = hashlib.sha256()
     
        with open(file_path, 'rb') as f:
            while chunk := f.read(8192):
 sha256.update(chunk)
        
        return sha256.hexdigest()
 
def generate_integrity_manifest(self):
        """Gera manifesto de integridade para scripts"""
        self.log("Gerando manifesto de integridade...", "INFO")
 
   manifest = {
            'generated_at': datetime.now().isoformat(),
         'files': {}
 }
        
        for file_path in SCRIPTS_PATH.rglob("*.py"):
          rel_path = str(file_path.relative_to(WORKSPACE_ROOT))
   file_hash = self.calculate_hash(file_path)
  
            manifest['files'][rel_path] = {
                'hash': file_hash,
 'size': file_path.stat().st_size,
            'modified': datetime.fromtimestamp(file_path.stat().st_mtime).isoformat()
            }
        
      with open(MANIFEST_FILE, 'w', encoding='utf-8') as f:
   json.dump(manifest, f, indent=2, ensure_ascii=False)
        
   self.log(f"Manifesto gerado: {len(manifest['files'])} arquivos", "INFO")
      
        self.report_data['metrics']['integrity_manifest_files'] = len(manifest['files'])
    
    def verify_integrity(self):
        """Verifica integridade dos scripts contra manifesto"""
      if not MANIFEST_FILE.exists():
         self.log("Manifesto não encontrado. Gerando novo...", "WARNING")
            self.generate_integrity_manifest()
      return
        
    self.log("Verificando integridade...", "INFO")
        
        with open(MANIFEST_FILE, 'r', encoding='utf-8') as f:
            manifest = json.load(f)
        
mismatches = []
        
        for file_rel_path, file_data in manifest['files'].items():
            file_path = WORKSPACE_ROOT / file_rel_path
      
            if not file_path.exists():
    mismatches.append(f"Arquivo removido: {file_rel_path}")
      continue

            current_hash = self.calculate_hash(file_path)
  
         if current_hash != file_data['hash']:
          mismatches.append(f"Hash divergente: {file_rel_path}")
        
     if mismatches:
   self.log(f"ALERTA: {len(mismatches)} divergências encontradas!", "ERROR")
     self.report_data['issues'].extend(mismatches)
        else:
      self.log("Integridade verificada: OK", "INFO")
        
   self.report_data['metrics']['integrity_mismatches'] = len(mismatches)
    
    # ═══════════════════════════════════════════════════════════════════════
    # MÉTRICAS
    # ═══════════════════════════════════════════════════════════════════════
    
    def collect_metrics(self):
"""Coleta métricas gerais"""
        self.log("Coletando métricas...", "INFO")

        # Total de arquivos
        total_files = sum(1 for _ in WORKSPACE_ROOT.rglob("*") if _.is_file())
        
     # Tamanho total
        total_size = sum(f.stat().st_size for f in WORKSPACE_ROOT.rglob("*") if f.is_file())
 total_size_mb = total_size / 1024 / 1024
        
        # Conversas salvas
      conversas_count = len(list((DOCS_PATH / "Relatorios" / "Conversas").glob("*.md")))
        
        # Logs arquivados
      archived_logs = len(list((LOGS_PATH / "Archive").glob("*.log")))
        
      self.report_data['metrics'].update({
   'total_files': total_files,
    'total_size_mb': round(total_size_mb, 2),
            'conversas_count': conversas_count,
            'archived_logs_count': archived_logs
 })
        
        self.log(f"Métricas coletadas: {total_files} arquivos, {total_size_mb:.2f} MB", "INFO")
    
    # ═══════════════════════════════════════════════════════════════════════
    # RELATÓRIO
    # ═══════════════════════════════════════════════════════════════════════
    
    def generate_report(self):
        """Gera relatório de conformidade em Markdown"""
        self.log("Gerando relatório...", "INFO")
        
        report_path = DOCS_PATH / "Relatorios" / "Auditorias" / f"AUDITORIA_ARCHIVUS_{datetime.now().strftime('%Y-%m-%d')}.md"
   report_path.parent.mkdir(parents=True, exist_ok=True)
  
  report_content = f"""# 📚 Relatório de Auditoria - Agente Archivus

**Data:** {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}  
**Agente:** Bibliotecário (Archivus)  
**Workspace:** {WORKSPACE_ROOT}

---

## 📊 Métricas Gerais

| Métrica | Valor |
|---------|-------|
| Total de arquivos | {self.report_data['metrics'].get('total_files', 0)} |
| Tamanho total | {self.report_data['metrics'].get('total_size_mb', 0)} MB |
| Conversas salvas | {self.report_data['metrics'].get('conversas_count', 0)} |
| Logs arquivados | {self.report_data['metrics'].get('archived_logs_count', 0)} |
| Arquivos no manifesto | {self.report_data['metrics'].get('integrity_manifest_files', 0)} |

---

## ⚙️ Ações Executadas

"""
        
   for action in self.report_data['actions']:
      report_content += f"### {action['action']}\n\n"
            for key, value in action.items():
     if key != 'action':
           report_content += f"- **{key}:** {value}\n"
       report_content += "\n"
     
        if self.report_data['issues']:
            report_content += f"""---

## ⚠️ Problemas Detectados

"""
     for issue in self.report_data['issues']:
       report_content += f"- {issue}\n"
        else:
 report_content += f"""---

## ✅ Status

Nenhum problema detectado. Sistema em conformidade.
"""
        
   report_content += f"""
---

**Gerado automaticamente por:** Agente Archivus  
**Próxima execução:** {(datetime.now() + timedelta(days=1)).strftime("%Y-%m-%d 02:00")}
"""
        
 with open(report_path, 'w', encoding='utf-8') as f:
    f.write(report_content)
        
        self.log(f"Relatório salvo: {report_path.name}", "INFO")
    
    # ═══════════════════════════════════════════════════════════════════════
    # EXECUÇÃO PRINCIPAL
    # ═══════════════════════════════════════════════════════════════════════
    
    def run_daily_tasks(self):
        """Executa todas as rotinas diárias"""
        self.log("═" * 60, "INFO")
        self.log("ARCHIVUS - ROTINA DIÁRIA INICIADA", "INFO")
        self.log("═" * 60, "INFO")
  
      try:
            self.validate_structure()
            self.rotate_logs()
            self.backup_scripts()
       self.verify_integrity()
    self.collect_metrics()
    self.generate_report()
            
        self.log("═" * 60, "INFO")
  self.log("ROTINA DIÁRIA CONCLUÍDA COM SUCESSO", "INFO")
   self.log("═" * 60, "INFO")
            
        except Exception as e:
   self.log(f"ERRO DURANTE EXECUÇÃO: {str(e)}", "ERROR")
            raise

# ═══════════════════════════════════════════════════════════════════════════
# EXECUÇÃO
# ═══════════════════════════════════════════════════════════════════════════

if __name__ == "__main__":
    archivus = Archivus()
    archivus.run_daily_tasks()
