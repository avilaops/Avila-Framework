# üßÆ √ÅVILA - FUNDAMENTOS MATEM√ÅTICOS

## Documento T√©cnico: Matem√°tica por Tr√°s da Orquestra√ß√£o

**Data:** 2025-11-10
**Autor:** √Åvila Tech
**√Årea:** Machine Learning & AI Engineering

---

## üìê 1. EMBEDDINGS & ESPA√áO VETORIAL

### 1.1 Representa√ß√£o de Documentos

Cada documento `d` √© transformado em um vetor `v ‚àà ‚Ñù‚Åø`:

```
d = "Este √© um documento sobre Azure"
     ‚Üì [Modelo de Embedding]
v = [0.234, -0.891, 0.445, ..., 0.123] ‚àà ‚Ñù¬π‚Åµ¬≥‚Å∂
```

**Propriedades do Espa√ßo Vetorial:**
- **Dimensionalidade:** n = 1536 (OpenAI Ada-003) ou 384 (MiniLM)
- **Norma:** Vetores normalizados (‚Äñv‚Äñ = 1)
- **Espa√ßo:** M√©trico com dist√¢ncia euclidiana ou cosseno

### 1.2 Similaridade Cosseno

Mede o √¢ngulo entre dois vetores (independente da magnitude):

```
                    v‚ÇÅ ¬∑ v‚ÇÇ
cos(Œ∏) = ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
          ‚Äñv‚ÇÅ‚Äñ √ó ‚Äñv‚ÇÇ‚Äñ

Onde:
- v‚ÇÅ ¬∑ v‚ÇÇ = Œ£(v‚ÇÅ·µ¢ √ó v‚ÇÇ·µ¢)  [produto escalar]
- ‚Äñv‚Äñ = ‚àö(Œ£ v·µ¢¬≤)           [norma L2]

Intervalo: [-1, 1]
  1.0  ‚Üí id√™nticos
  0.0  ‚Üí ortogonais (sem rela√ß√£o)
 -1.0  ‚Üí opostos
```

**Implementa√ß√£o Otimizada (NumPy):**
```python
def cosine_similarity(v1, v2):
    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))

# Vetorizada para batch:
def cosine_similarity_matrix(V):
    # V: matriz (n_docs √ó n_dims)
    V_norm = V / np.linalg.norm(V, axis=1, keepdims=True)
    return V_norm @ V_norm.T  # Produto matricial
    # Complexidade: O(n¬≤d) onde d = dimens√£o
```

### 1.3 Dist√¢ncia Euclidiana

Alternativa √† similaridade cosseno:

```
                _______________
d(v‚ÇÅ, v‚ÇÇ) = ‚àö Œ£ (v‚ÇÅ·µ¢ - v‚ÇÇ·µ¢)¬≤
               i=1..n

Propriedades:
- M√©trica: d(x,y) ‚â• 0, d(x,x) = 0, sim√©trica, desigualdade triangular
- Sens√≠vel a magnitude (diferente do cosseno)
```

**Convers√£o entre M√©tricas:**
```
Para vetores normalizados (‚Äñv‚Äñ = 1):

d¬≤_euclidean(v‚ÇÅ, v‚ÇÇ) = 2 √ó (1 - cos(v‚ÇÅ, v‚ÇÇ))

Portanto:
cos_sim = 1 - (d¬≤_euclidean / 2)
```

---

## üîç 2. DETEC√á√ÉO DE DUPLICATAS

### 2.1 Threshold-Based Approach

Simples e interpret√°vel:

```
duplicatas(d‚ÇÅ, d‚ÇÇ) = {
    True   se cos(v‚ÇÅ, v‚ÇÇ) ‚â• œÑ
    False  caso contr√°rio
}

Onde œÑ = threshold (tipicamente 0.85)

Trade-off:
- œÑ alto (0.95): Poucos falsos positivos, mais falsos negativos
- œÑ baixo (0.75): Mais falsos positivos, poucos falsos negativos
```

**Otimiza√ß√£o: Precision-Recall Curve**
```
Precision = TP / (TP + FP)
Recall    = TP / (TP + FN)

F1-Score = 2 √ó (Precision √ó Recall) / (Precision + Recall)

Escolher œÑ que maximiza F1-Score em conjunto de valida√ß√£o
```

### 2.2 Locality-Sensitive Hashing (LSH)

Para escalar a milh√µes de documentos:

**Problema:** Comparar todos os pares = O(n¬≤) ‚Üí invi√°vel

**Solu√ß√£o:** Hash que preserva similaridade

```
Princ√≠pio:
Se cos(v‚ÇÅ, v‚ÇÇ) alto ‚Üí P(hash(v‚ÇÅ) = hash(v‚ÇÇ)) alto

Implementa√ß√£o (SimHash):
1. Para cada dimens√£o i:
   - Se v·µ¢ > 0: bit = 1
   - Se v·µ¢ ‚â§ 0: bit = 0

2. Hash = concatena√ß√£o de bits

3. Hamming distance entre hashes ‚âà Angular distance entre vetores

Complexidade: O(n log n) [busca em hash table]
```

**C√≥digo Python:**
```python
import numpy as np
from collections import defaultdict

def simhash(vector, num_bits=128):
    """Gera hash bin√°rio de 128 bits"""
    # Proje√ß√µes aleat√≥rias
    random_vectors = np.random.randn(len(vector), num_bits)
    projections = vector @ random_vectors
    return tuple(projections > 0)  # Boolean array

def find_candidates(vectors, threshold=0.85):
    """Encontra pares candidatos a duplicatas"""
    hash_buckets = defaultdict(list)

    # Indexar por hash
    for i, v in enumerate(vectors):
        h = simhash(v)
        hash_buckets[h].append(i)

    # Pares no mesmo bucket s√£o candidatos
    candidates = []
    for bucket in hash_buckets.values():
        if len(bucket) > 1:
            # Verificar similaridade exata
            for i in range(len(bucket)):
                for j in range(i+1, len(bucket)):
                    sim = cosine_similarity(vectors[bucket[i]], vectors[bucket[j]])
                    if sim >= threshold:
                        candidates.append((bucket[i], bucket[j], sim))

    return candidates
```

---

## üìä 3. CLUSTERING HIER√ÅRQUICO

### 3.1 Agglomerative Clustering

Abordagem bottom-up:

```
Algoritmo:
1. Iniciar: cada documento = 1 cluster
2. Repetir at√© k clusters:
   a) Encontrar par de clusters mais pr√≥ximos
   b) Mesclar em novo cluster
   c) Atualizar dist√¢ncias

Complexidade: O(n¬≥) naive, O(n¬≤ log n) otimizado
```

**Medidas de Linkage:**

1. **Single Linkage** (M√≠nimo)
   ```
   d(C‚ÇÅ, C‚ÇÇ) = min{d(x, y) : x ‚àà C‚ÇÅ, y ‚àà C‚ÇÇ}

   ‚Üí Clusters em "cadeia"
   ‚Üí Sens√≠vel a outliers
   ```

2. **Complete Linkage** (M√°ximo)
   ```
   d(C‚ÇÅ, C‚ÇÇ) = max{d(x, y) : x ‚àà C‚ÇÅ, y ‚àà C‚ÇÇ}

   ‚Üí Clusters compactos
   ‚Üí Mais robusto
   ```

3. **Ward Linkage** (Vari√¢ncia) ‚≠ê **RECOMENDADO**
   ```
   d(C‚ÇÅ, C‚ÇÇ) = Œ£ ‚Äñx·µ¢ - Œº‚Äñ¬≤ ap√≥s merge
              i‚ààC‚ÇÅ‚à™C‚ÇÇ

   Onde Œº = centroid do cluster mesclado

   ‚Üí Minimiza vari√¢ncia intra-cluster
   ‚Üí Balanceado e robusto
   ```

**Implementa√ß√£o (scikit-learn):**
```python
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage
import matplotlib.pyplot as plt

# Clustering
model = AgglomerativeClustering(
    n_clusters=None,  # Autom√°tico
    distance_threshold=0.5,  # Cortar em dist√¢ncia
    linkage='ward'
)
labels = model.fit_predict(embeddings)

# Dendrograma
linkage_matrix = linkage(embeddings, method='ward')
plt.figure(figsize=(12, 8))
dendrogram(linkage_matrix, truncate_mode='lastp', p=30)
plt.title('Dendrograma Hier√°rquico')
plt.xlabel('Cluster ID')
plt.ylabel('Dist√¢ncia')
plt.show()
```

### 3.2 HDBSCAN (Density-Based)

Clustering baseado em densidade ‚Üí encontra k automaticamente:

```
Princ√≠pio:
- Clusters = regi√µes de alta densidade
- Outliers = pontos em regi√µes de baixa densidade

Vantagens:
‚úì N√£o precisa especificar k
‚úì Detecta outliers
‚úì Clusters de formas arbitr√°rias

Desvantagens:
‚úó Mais lento (O(n log n))
‚úó Sens√≠vel a hiperpar√¢metros (min_cluster_size)
```

**Hiperpar√¢metros:**
- `min_cluster_size`: M√≠nimo de pontos para formar cluster (ex: 5)
- `min_samples`: Densidade necess√°ria (ex: 3)

**C√≥digo:**
```python
import hdbscan

clusterer = hdbscan.HDBSCAN(
    min_cluster_size=5,
    min_samples=3,
    metric='euclidean',
    cluster_selection_method='eom'  # excess of mass
)
labels = clusterer.fit_predict(embeddings)

# Outliers t√™m label = -1
num_outliers = np.sum(labels == -1)
print(f"Outliers: {num_outliers} ({100*num_outliers/len(labels):.1f}%)")
```

### 3.3 N√∫mero √ìtimo de Clusters

**M√©todo 1: Silhouette Score**
```
Para cada ponto i:
  a(i) = dist√¢ncia m√©dia a pontos no mesmo cluster
  b(i) = dist√¢ncia m√©dia ao cluster mais pr√≥ximo

  s(i) = (b(i) - a(i)) / max(a(i), b(i))

Silhouette = m√©dia de s(i)

Intervalo: [-1, 1]
  1  ‚Üí bem clusterizado
  0  ‚Üí na fronteira
 -1  ‚Üí no cluster errado

Escolher k que maximiza Silhouette
```

**M√©todo 2: Davies-Bouldin Index**
```
Para cada par de clusters (i, j):
  œÉ·µ¢ = dispers√£o do cluster i
  œÉ‚±º = dispers√£o do cluster j
  d(c·µ¢, c‚±º) = dist√¢ncia entre centroids

  R·µ¢‚±º = (œÉ·µ¢ + œÉ‚±º) / d(c·µ¢, c‚±º)

DB = (1/k) Œ£ max(R·µ¢‚±º)
            i  j‚â†i

Menor DB = melhor (clusters compactos e separados)
```

**M√©todo 3: Elbow Method**
```
In√©rcia(k) = Œ£  Œ£  ‚Äñx - Œº‚Çñ‚Äñ¬≤
            k x‚ààC‚Çñ

Plot In√©rcia vs k ‚Üí "cotovelo" indica k √≥timo
```

**Implementa√ß√£o:**
```python
from sklearn.metrics import silhouette_score, davies_bouldin_score

def find_optimal_k(embeddings, k_range=(2, 20)):
    scores = []
    for k in range(k_range[0], k_range[1]+1):
        model = AgglomerativeClustering(n_clusters=k)
        labels = model.fit_predict(embeddings)

        sil = silhouette_score(embeddings, labels)
        db = davies_bouldin_score(embeddings, labels)

        scores.append({
            'k': k,
            'silhouette': sil,
            'davies_bouldin': db
        })

    # Melhor k: maior silhouette, menor DB
    best = max(scores, key=lambda x: x['silhouette'])
    return best['k'], scores
```

---

## üï∏Ô∏è 4. KNOWLEDGE GRAPHS

### 4.1 Representa√ß√£o

**Grafo Direcionado Ponderado:**
```
G = (V, E, W)

V = {d‚ÇÅ, d‚ÇÇ, ..., d‚Çô}  [documentos]
E ‚äÜ V √ó V               [rela√ß√µes]
W: E ‚Üí [0, 1]           [pesos = similaridade]

Exemplo:
(d‚ÇÅ, d‚ÇÇ, 0.87) significa: d‚ÇÅ relacionado a d‚ÇÇ com 87% de similaridade
```

**Matriz de Adjac√™ncia:**
```
A ‚àà ‚Ñù‚ÅøÀ£‚Åø

A·µ¢‚±º = {
    w(d·µ¢, d‚±º)  se (d·µ¢, d‚±º) ‚àà E
    0          caso contr√°rio
}

Propriedades:
- Sim√©trica (grafo n√£o-direcionado)
- Diagonal zero (sem self-loops)
- Esparsa (poucas conex√µes)
```

### 4.2 PageRank

Calcula "import√¢ncia" de cada documento:

```
F√≥rmula Iterativa:

PR(d·µ¢) = (1-Œ±)/n + Œ± ¬∑ Œ£ PR(d‚±º) / L(d‚±º)
                       j‚Üíi

Onde:
- Œ± = damping factor (0.85) [prob de seguir link]
- n = n√∫mero total de documentos
- L(d‚±º) = n√∫mero de links saindo de d‚±º
- j‚Üíi = todos j que linkam para i

Converg√™ncia:
|PR^(t+1) - PR^(t)| < Œµ  (tipicamente Œµ = 0.0001)
```

**Forma Matricial:**
```
PR = (1-Œ±) ¬∑ 1/n ¬∑ 1‚Åø + Œ± ¬∑ M ¬∑ PR

Onde M √© a matriz de transi√ß√£o:
M·µ¢‚±º = A·µ¢‚±º / Œ£‚Çñ A·µ¢‚Çñ

Solu√ß√£o: autovetor correspondente ao autovalor Œª=1
```

**Implementa√ß√£o (NetworkX):**
```python
import networkx as nx

# Criar grafo
G = nx.Graph()
for i, doc in enumerate(documents):
    G.add_node(i, title=doc['title'], category=doc['category'])

# Adicionar arestas
for i in range(len(embeddings)):
    for j in range(i+1, len(embeddings)):
        sim = cosine_similarity(embeddings[i], embeddings[j])
        if sim > 0.65:  # Threshold
            G.add_edge(i, j, weight=sim)

# PageRank
pr = nx.pagerank(G, alpha=0.85, max_iter=100, tol=1e-6)

# Documentos mais importantes
top_docs = sorted(pr.items(), key=lambda x: x[1], reverse=True)[:10]
for doc_id, score in top_docs:
    print(f"{documents[doc_id]['title']}: {score:.4f}")
```

### 4.3 Community Detection

Encontrar grupos de documentos densamente conectados:

**Algoritmo de Louvain:**
```
Objetivo: Maximizar Modularidade

         1     ‚é°          k·µ¢k‚±º ‚é§
Q = ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Œ£ ‚é¢ A·µ¢‚±º - ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚é• Œ¥(c·µ¢, c‚±º)
      2m    i,j ‚é£          2m   ‚é¶

Onde:
- m = n√∫mero de arestas
- k·µ¢ = grau do n√≥ i
- Œ¥(c·µ¢, c‚±º) = 1 se i e j na mesma comunidade, 0 caso contr√°rio
- A·µ¢‚±º = peso da aresta

Q ‚àà [-0.5, 1]
  Q > 0.3 ‚Üí estrutura de comunidade forte
```

**Implementa√ß√£o:**
```python
from networkx.algorithms import community

# Louvain
communities = community.louvain_communities(G, weight='weight', resolution=1.0)

# Modularidade
modularity = community.modularity(G, communities, weight='weight')
print(f"Modularidade: {modularity:.3f}")

# Visualizar
import matplotlib.pyplot as plt
pos = nx.spring_layout(G, k=0.5, iterations=50)
colors = [0] * G.number_of_nodes()
for i, comm in enumerate(communities):
    for node in comm:
        colors[node] = i

nx.draw(G, pos, node_color=colors, cmap=plt.cm.rainbow,
        node_size=100, with_labels=False, edge_color='gray', alpha=0.6)
plt.show()
```

### 4.4 Shortest Path

Encontrar "caminho de conhecimento" entre documentos:

```
Dijkstra's Algorithm:

dist[s] = 0
dist[v] = ‚àû para v ‚â† s

Q = priority_queue(all nodes by dist)

while Q not empty:
    u = extract_min(Q)
    for each neighbor v of u:
        alt = dist[u] + weight(u, v)
        if alt < dist[v]:
            dist[v] = alt
            prev[v] = u

Complexidade: O(|E| + |V| log |V|) com heap
```

**Uso:**
```python
# Caminho entre dois documentos
path = nx.shortest_path(G, source=doc1_id, target=doc2_id, weight='weight')
print("Caminho de conhecimento:")
for node in path:
    print(f"  ‚Üí {documents[node]['title']}")

# Todos os caminhos mais curtos
lengths = nx.shortest_path_length(G, weight='weight')
avg_path_length = sum(sum(l.values()) for l in lengths.values()) / (n * (n-1))
print(f"Caminho m√©dio: {avg_path_length:.2f}")
```

---

## üìà 5. MODELOS DE PRODUTIVIDADE

### 5.1 Time Series Analysis

**Decomposi√ß√£o:**
```
Y‚Çú = Trend‚Çú + Seasonality‚Çú + Noise‚Çú

Onde:
- Trend: tend√™ncia de longo prazo
- Seasonality: padr√µes c√≠clicos (di√°rio, semanal)
- Noise: varia√ß√µes aleat√≥rias

M√©todo: STL (Seasonal-Trend decomposition using Loess)
```

**ARIMA para Previs√£o:**
```
ARIMA(p, d, q):
  p = ordem autoregressiva
  d = grau de diferencia√ß√£o
  q = ordem m√©dia m√≥vel

Modelo:
Y‚Çú = c + œÜ‚ÇÅY‚Çú‚Çã‚ÇÅ + ... + œÜ‚ÇöY‚Çú‚Çã‚Çö + Œ∏‚ÇÅŒµ‚Çú‚Çã‚ÇÅ + ... + Œ∏·µßŒµ‚Çú‚Çã·µß + Œµ‚Çú

Auto-sele√ß√£o: auto.arima() testa m√∫ltiplas combina√ß√µes
```

**C√≥digo:**
```python
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.arima.model import ARIMA
import pandas as pd

# Dados: horas de trabalho por dia
df = pd.read_csv('productivity.csv', parse_dates=['date'])
df.set_index('date', inplace=True)

# Decomposi√ß√£o
decomposition = seasonal_decompose(df['hours'], model='additive', period=7)
decomposition.plot()

# ARIMA
model = ARIMA(df['hours'], order=(1, 1, 1))
fitted = model.fit()

# Previs√£o pr√≥ximos 7 dias
forecast = fitted.forecast(steps=7)
print(forecast)
```

### 5.2 LSTM para S√©ries Temporais

Deep learning para capturar padr√µes complexos:

**Arquitetura:**
```
Input: [activity_t-n, ..., activity_t-1]
       shape = (batch, timesteps, features)

LSTM Cell:
  f‚Çú = œÉ(Wf ¬∑ [h‚Çú‚Çã‚ÇÅ, x‚Çú] + bf)  [forget gate]
  i‚Çú = œÉ(Wi ¬∑ [h‚Çú‚Çã‚ÇÅ, x‚Çú] + bi)  [input gate]
  CÃÉ‚Çú = tanh(Wc ¬∑ [h‚Çú‚Çã‚ÇÅ, x‚Çú] + bc)  [candidate]
  C‚Çú = f‚Çú ‚äô C‚Çú‚Çã‚ÇÅ + i‚Çú ‚äô CÃÉ‚Çú  [cell state]
  o‚Çú = œÉ(Wo ¬∑ [h‚Çú‚Çã‚ÇÅ, x‚Çú] + bo)  [output gate]
  h‚Çú = o‚Çú ‚äô tanh(C‚Çú)  [hidden state]

Output: produtividade prevista em t+1
```

**Implementa√ß√£o (PyTorch):**
```python
import torch
import torch.nn as nn

class ProductivityLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers=2):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,
                            batch_first=True, dropout=0.2)
        self.fc = nn.Linear(hidden_size, 1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        # x: (batch, seq_len, features)
        out, (h, c) = self.lstm(x)
        # Pegar √∫ltimo timestep
        out = self.fc(out[:, -1, :])
        out = self.sigmoid(out)
        return out

# Treinamento
model = ProductivityLSTM(input_size=10, hidden_size=64)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

for epoch in range(100):
    for batch in dataloader:
        X, y = batch
        pred = model(X)
        loss = criterion(pred, y)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```

### 5.3 Anomaly Detection

Detectar dias anormalmente improdutivos:

**Z-Score:**
```
z = (x - Œº) / œÉ

Se |z| > 3 ‚Üí outlier (99.7% dos dados em ¬±3œÉ)

Vantagens: Simples, interpret√°vel
Desvantagens: Assume distribui√ß√£o normal
```

**Isolation Forest:**
```
Princ√≠pio:
- Anomalias s√£o "f√°ceis de isolar"
- Construir √°rvores aleat√≥rias
- Anomalias t√™m path length curto

Score:
s(x) = 2^(-E[h(x)] / c(n))

s(x) ‚âà 1 ‚Üí anomalia
s(x) << 0.5 ‚Üí normal
```

**Implementa√ß√£o:**
```python
from sklearn.ensemble import IsolationForest

# Treinar
clf = IsolationForest(contamination=0.05, random_state=42)
clf.fit(features)  # shape: (n_days, n_features)

# Detectar anomalias
predictions = clf.predict(features)
anomalies = predictions == -1

# Investigar
anomaly_days = df[anomalies]
print("Dias an√¥malos:")
print(anomaly_days[['date', 'hours', 'deep_work_ratio']])
```

---

## üí∞ 6. OTIMIZA√á√ÉO DE CUSTOS AZURE

### 6.1 Modelo de Custo

```
Custo Total = Œ£ (pre√ßo_i √ó quantidade_i √ó tempo_i)
             i‚ààrecursos

Recursos comuns:
- VMs: $0.096/hora (B2s)
- Storage: $0.02/GB/m√™s
- Bandwidth: $0.087/GB (sa√≠da)
- Functions: $0.20/milh√£o execu√ß√µes
```

### 6.2 Problema de Otimiza√ß√£o

```
minimize: C = Œ£ c·µ¢x·µ¢
             i

subject to:
  Performance(x) ‚â• SLA_min
  Latency(x) ‚â§ L_max
  x·µ¢ ‚â• 0 ‚àÄi
  x·µ¢ ‚àà ‚Ñ§ (se VM count)

Onde:
- c·µ¢ = custo do recurso i
- x·µ¢ = quantidade do recurso i
```

**Abordagem: Linear Programming**
```python
from scipy.optimize import linprog

# Coeficientes de custo
c = [0.096, 0.02, 0.087]  # [VM/hora, Storage/GB, Bandwidth/GB]

# Inequa√ß√µes (Ax ‚â§ b)
A = [
    [-100, 0, 0],  # Performance: -100*VM ‚â§ -1000 ‚Üí VM ‚â• 10
    [50, 0, 0],    # Latency: 50*VM ‚â§ 500 ‚Üí VM ‚â§ 10
]
b = [-1000, 500]

# Limites
x_bounds = [(0, None), (0, None), (0, None)]

# Resolver
result = linprog(c, A_ub=A, b_ub=b, bounds=x_bounds, method='highs')
print(f"Configura√ß√£o √≥tima: {result.x}")
print(f"Custo m√≠nimo: ${result.fun:.2f}")
```

### 6.3 Previs√£o de Custos

**Prophet (Facebook):**
```python
from prophet import Prophet

# Hist√≥rico de custos
df = pd.DataFrame({
    'ds': date_range,  # datas
    'y': costs         # custos di√°rios
})

# Treinar
model = Prophet(
    yearly_seasonality=False,
    weekly_seasonality=True,
    daily_seasonality=False
)
model.fit(df)

# Prever pr√≥ximos 30 dias
future = model.make_future_dataframe(periods=30)
forecast = model.predict(future)

# Alertar se previs√£o > or√ßamento
budget = 1000  # $/m√™s
predicted_monthly = forecast['yhat'].tail(30).sum()
if predicted_monthly > budget:
    print(f"‚ö†Ô∏è  ALERTA: Custo previsto ${predicted_monthly:.2f} excede or√ßamento!")
```

---

## üé≤ 7. PROBABILIDADE & ESTAT√çSTICA

### 7.1 Distribui√ß√µes Importantes

**Normal (Gaussian):**
```
f(x | Œº, œÉ¬≤) = (1/‚àö(2œÄœÉ¬≤)) ¬∑ exp(-(x-Œº)¬≤/(2œÉ¬≤))

Usado para: m√©tricas de produtividade, tempos de resposta
```

**Poisson:**
```
P(X = k) = (Œª·µè e^(-Œª)) / k!

Usado para: contagem de eventos (commits/dia, deploys/semana)
```

**Exponencial:**
```
f(x | Œª) = Œª e^(-Œªx)  para x ‚â• 0

Usado para: tempo entre eventos (intervalo entre deploys)
```

### 7.2 Testes de Hip√≥tese

**A/B Testing:**
```
H‚ÇÄ: Œº_A = Œº_B  (n√£o h√° diferen√ßa)
H‚ÇÅ: Œº_A ‚â† Œº_B  (h√° diferen√ßa)

t-test:
t = (xÃÑ_A - xÃÑ_B) / ‚àö(s¬≤_A/n_A + s¬≤_B/n_B)

Se |t| > t_cr√≠tico (Œ±=0.05) ‚Üí rejeitar H‚ÇÄ
```

**Implementa√ß√£o:**
```python
from scipy.stats import ttest_ind

# Produtividade antes e depois de mudan√ßa
before = [7.2, 6.8, 7.5, 7.0, 6.9]
after = [8.1, 8.3, 7.9, 8.2, 8.0]

t_stat, p_value = ttest_ind(before, after)
if p_value < 0.05:
    print(f"Mudan√ßa significativa! (p = {p_value:.4f})")
else:
    print("Sem evid√™ncia de mudan√ßa significativa")
```

---

## üß© 8. TEORIA DA INFORMA√á√ÉO

### 8.1 Entropia

Mede "surpresa" / incerteza:

```
H(X) = -Œ£ p(x) log‚ÇÇ p(x)
        x

Exemplo: Categorias de documentos
P(Trabalho) = 0.5
P(Aprendizado) = 0.3
P(Ferramentas) = 0.2

H = -0.5¬∑log‚ÇÇ(0.5) - 0.3¬∑log‚ÇÇ(0.3) - 0.2¬∑log‚ÇÇ(0.2)
  = 1.49 bits

M√°xima entropia: log‚ÇÇ(n) quando todas categorias equiprov√°veis
```

### 8.2 Informa√ß√£o M√∫tua

Quanto uma vari√°vel "informa" sobre outra:

```
I(X;Y) = Œ£  Œ£  p(x,y) log‚ÇÇ(p(x,y) / (p(x)p(y)))
         x  y

I(X;Y) = 0 ‚Üí independentes
I(X;Y) = H(X) = H(Y) ‚Üí perfeitamente correlacionadas

Uso: Feature selection (selecionar features com alta MI com target)
```

---

## üìö REFER√äNCIAS

1. **Linear Algebra:** Gilbert Strang - "Linear Algebra and Its Applications"
2. **Machine Learning:** Bishop - "Pattern Recognition and Machine Learning"
3. **Information Retrieval:** Manning et al. - "Introduction to Information Retrieval"
4. **Time Series:** Hyndman & Athanasopoulos - "Forecasting: Principles and Practice"
5. **Graph Theory:** Newman - "Networks: An Introduction"
6. **Deep Learning:** Goodfellow et al. - "Deep Learning"

---

**üßÆ "Na matem√°tica, encontramos a linguagem universal da intelig√™ncia."**

---

*Atualizar com novos m√©todos conforme implementados*
